% Linear methods, LRec and SLIM, work very well in practice. However, they require solving multiple linear regression for a very large design matrix $R$. 

%Although the algorithm is distributable, the number of users and items can be in the order of millions in a real world problems making it computationally expensive. 
% Despite of superior performance, LRec's memory requirement is quadratic and thus limiting its applicability in real-world large scale recommendation.
% Despite LRec's superior performance, the memory requirement is quadratic which limits its applicability in a large scale datasets.

% \subsubsection{Via SVD}
Despite their superior performance, the applicability of Linear methods on real world large scale dataset are constrained by their computational cost. Linear methods involve solving  a large number of regression subproblems on a huge design matrix $\R$ making it extremely challenging on real world applications where the number of users and items is in millions.

Here we propose a model and an algorithm for scaling up linear methods to large OC-CF problems. 
%Our approach hinges on a few key ideas, which may have appeared in isolation in other contexts but their combined use in the context of OC-CF is, to the best of our knowledge, novel. 
In particular, we are seeking an approximation $\R \approx \R \W$ that attempts to capture most of the row space of the matrix $\R$ through a matrix $\W$ that is low-rank {\em and} has small Frobenius norm. The motivation for such a double regularization is to better control the generalization error of the model, an insight that was proven correct by our experiments. Moreover, it turns out that there is a very natural and efficient way to compute such an approximate decomposition of the matrix $\R$ by randomization~\citep{halko2011}, which allows scaling to large problems.

Our model amounts to solving the following optimization problem
 \begin{align}
\label{eqn:reg-rank}
\underset{\mbox{\footnotesize rank}(\W) \le k}{\mathrm{argmin}} & \left \| \R - \R\W\right \|_F^2 + \lambda \left \|  \W \right \|_F^2
\end{align}
where, typically, $k\ \ll \ n$.
% $ I \in \mathbb{R}^{n \times n}$ would be the trivial solution for W, however, the rank constraint prevents W from the trivial solution as $\{rank(I) = n\} > p $.
% Even though we can't directly solve the eqn~\ref{eqn:reg-rank}, the optimal solution when $\lambda = 0$ is given by SVD as 
For $\lambda = 0$, the optimal solution is given by the Eckart-Young theorem (see, e.g.,~\citep{halko2011})
   \begin{align}
   \label{eqn:SVDOptimal}
   \W = \Q_k \Q_k^ T
   \end{align}
where  $\Q_k$ is an orthogonal matrix computed by a truncated SVD 
   \begin{align*}
   % R =\ P \Sig Q^T \\
   \R \approx \P_k \Sig_k \Q_k^T
   \end{align*}
Similarly, if we drop the low-rank constraint in (\ref{eqn:reg-rank}), the optimal matrix $\W$ is given by the solution of a standard regression problem 
%   First, we define the $L_2$ regularized linear model 
%\begin{equation}
%\label{eqn:squaredLinear}
%\underset{\W}{\mathrm{argmin}}|| \R - \R \W ||_F^2 + \frac{\lambda}{2} || \W ||_F^2 
%% \inf_{ \W  } || \R - \R \W ||_F^2 + \frac{\lambda}{2} || \W ||_F^2 
%\end{equation}
%We observe that (\ref{eqn:squaredLinear}) is a  standard multiple linear regression problem with  
% We can solve Equation~\ref{eqn:squaredLinear} analytically as
\begin{equation}
\W = (\R^T \R + \lambda \I)^{-1} \R^T \R
\end{equation}
which involves the inverse of the original matrix $\R$ and therefore does not scale to large problems.
%However, the  memory requirement, computational cost and  numerical instability in computing the inverse, $(\R^T \R + \lambda \I)^{-1}$ , makes the analytical solution infeasible. Similarly, if we apply gradient based optimization, it becomes computationally expensive as it involves solving $n$ linear regression. \todob{This is a scaling up paper. Be precise about the computational cost.}
%% Gradient based optimization doesn't require computation of the inverse. However, it involves solving $n$ linear regression problem which is computationally expensive.

%
%However, the key limitation of the SVD solution is that it corresponds to an unregularized model, hence highly susceptible to overfitting. To incorporate model complexity control via regularization, 



However, under both a low-rank constraint and $\lambda > 0$ in (\ref{eqn:reg-rank}), finding the optimal $\W$ involves solving a hard nonconvex problem with no analytical solution in general. Nonetheless, an analytical solution is possible for a certain parametrization of $\W$ as we explain next. We first compute an approximate orthogonal basis $\Q_k$ of the row space of $\R$, i.e.,
\begin{equation}
\R \approx \R \Q_k \Q_k^T
\end{equation}
using randomized SVD. The randomized SVD algorithm is shown in Algorithm 1. (We refer to Halko et.~al~\citep{halko2011} for more details.)
Then we re-parametrize the matrix $\W$ as 
\begin{equation}
\W = \Q_k \Y
\end{equation}
for some matrix $\Y$. Note that through this parametrization the rank of $\W$ is automatically controlled, no optimality is lost when $\lambda=0$, and the optimization problem (\ref{eqn:reg-rank}) reads
\begin{align}
\label{eqn:i-reg-rank-low}
\underset{\Y} {\mathrm{argmin}}  \left \| \R - \R \Q_k \Y \right \|_F^2 + \lambda \left \|  \Q_k \Y \right \|_F^2 
\end{align}
Since $\Q_k$ is orthogonal, we have $\left \| \Q_k \Y \right \|_F = \left \|\Y\right \|_F$, and~(\ref{eqn:i-reg-rank-low}) becomes 
\begin{align}
% \begin{split}
\underset{\Y}{\mathrm{argmin}} \left \| \R - \R\Q_k\Y\right \|_F^2 + \lambda \left \|  \Y \right \|_F^2 
% \end{split}
\end{align}
The latter can be solved analytically to give 
\begin{align*}
\Y = (\Q_k^T \R^T\R\Q_k +\lambda I)^{-1} \Q_k^T \R^T \R
\end{align*}
Note that this inversion involves a $k \times k$ matrix, and hence it is tractable.

%Eq.~(\ref{eqn:i-reg-rank-low}) corresponds to the linear Autoencoder~\cite{Sedhain:2015} where the input-hidden layer weight is initialized with the rank-k orthogonal basis of the matrix $\R$. 

The choice of $\W = \Q_k \Y$ is motivated by the following observation. When $\lambda = 0$, the solution to our problem is $\W = \Q_k \Q_k^T$. This is also the solution to the new formulation of our problem for $\Y = \Q_k^T$. When $\lambda$ is close to zero, we believe that sufficiently good solutions lie close to the span of $\Q_k$. Therefore, we choose $\W = \Q_k \Y$. We demonstrate that this choice performs well empirically in the experimental section. Furthermore, \LinearLow can be seen as an autoencoder collaborative filtering model ~\cite{Sedhain:2015} with linear activation and $\Q_k$ as input-hidden weights.
 
We refer to (\ref{eqn:i-reg-rank-low}) as I-\LinearLow as it corresponds to item-item model. Similarly, we can define a user-user model, U-\LinearLow
\begin{align}
\label{eqn:u-reg-rank-low}
% \begin{split}
% \underset{rank(Y) = k}{\mathrm{argmin}}  \left \| R^T - R^TP_kY\right \|_F^2 + \lambda \left \|  P_kY \right \|_F^2 \\
\underset{\Y}{\mathrm{argmin}}  \left \| \R - \Y\P_k^T\R\right \|_F^2 + \lambda \left \|  \Y \right \|_F^2 
% \because  \left \|  P_kY \right \|_F^2 = \left \|  Y \right \|_F^2
% \end{split}
\end{align}


%Although low rank multiple-regression can be solved efficiently, it requires computationally expensive dimensionality reduction. However, the recent advances in randomized algorithms for approximate dimensionality reduction~\citep{halko2011} have made dimensionality reduction very efficient on large scale matrices. In this work, we use randomized SVD~\citep{halko2011} for low dimensional projection as summarized in Algorithm~\ref{algo:RSVD}.

As we discussed earlier, recommending similar items is very prevalent in real-world recommender systems.
In I-\LinearLow model, the item-item similarity is explicitly given by the matrix $\W = \Q_k \Y $. 


\begin{algorithm}
        % \algsetup{linenosize=\tiny}
        \small
        \caption{Given $R \in \mathbb{R}^{m \times n}$, compute approximate rank-k SVD; R $\approx$ $P_k \Sigma_k Q_k$}
        \label{algo:RSVD}
        \begin{algorithmic}[1]

        \Procedure{RSVD}{R, k}
        \State Draw $n\times k$ Gaussian random matrix $\Omega$
        \State Construct $n\times k$ sample matrix $A = R\Omega$
        \State Construct $m\times k$ orthonormal matrix $Z$, such that $A  = ZX$
        \State Constuct $k\times n$ matrix $B = Z^TR$
        \State Compute the SVD of B, B =  $\hat{P_k} \Sigma_k Q_k$
        \State  $R \Rightarrow ZB \Rightarrow Z \hat{P_k} \Sigma_k Q_k \Rightarrow  P_k \Sigma_k Q_k, where\ P_k = Z \hat{P_k}$
        % \State \begin{eqnarray*}
        %               R & = & ZB\\
        %               & = &  Z\hat{P_k}Σ_kQ_k\\
        %               & = & P_kΣ_kQ_k\\
        %               &   & where,\ P_k = Z\hat{P_k}\\
        %               \end{eqnarray*}
        \State return $P_k \Sigma_k Q_k$
        \EndProcedure
        \end{algorithmic}
\end{algorithm}


